<!DOCTYPE html>
<html lang="en">
<!-- css from https://github.com/ai-workshops/ai-workshops.github.io/blob/master/generalizable-policy-learning-in-the-physical-world/style.css -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
  integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">

<link rel="icon"
  href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text x=%22-.1em%22 y=%22.9em%22 font-size=%2280%22>ü§ñ</text></svg>">

<!-- jQuery library -->
<script src="https://code.jquery.com/jquery-3.5.1.min.js"
  integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

<!-- Latest compiled JavaScript -->
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.min.js"
  integrity="sha384-w1Q4orYjBQndcko6MimVbzY0tgp4pWB4lZ7lr30WKz0vr/aWKhXdBNmNb5D92v7s" crossorigin="anonymous"></script>



<head>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta charset="utf-8">
  <link rel="stylesheet" href="bootstrap/css/bootstrap.min.css">
  <title>LEAP: Learning Effective Abstractions for Planning | CoRL 2023</title>
  <link rel="stylesheet" href="css/style.css">

  <style>
    .collapsible {
      background-color: #777;
      color: white;
      cursor: pointer;
      padding: 18px;
      width: 100%;
      border: solid;
      text-align: left;
      outline: none;
      font-size: 15px;
    }

    .active, .collapsible:hover {
      background-color: #555;
    }

    .content {
      padding: 5px 18px;
      display: none;
      overflow: hidden;
      background-color: #f1f1f1;
    }

    .triangle-up {
      width: 0;
      height: 0;
      border-left: 10px solid transparent;
      border-right: 10px solid transparent;
      border-bottom: 20px solid rgb(255, 255, 255);
      float: right;
    }

    .triangle-down {
      width: 0;
      height: 0;
      border-left: 10px solid transparent;
      border-right: 10px solid transparent;
      border-top: 20px solid rgb(255, 255, 255);
      float: right;
    }

    .title-container {
      width: 100%;
      margin: auto;
      padding: 40px 20px;
      color: black;
      /* background-image: linear-gradient(135deg,#333, black, #333); */
      background:white;
      background-image: url("imgs/corl2023_background.png");
      background-size: cover;
      background-position: center;
      background-repeat: no-repeat, repeat;
      display: flex;
      flex-direction: column;
      justify-content: center;
      margin-top: 55px;
  }

  </style>


  <script type="text/javascript">
    var organizers = [
      {
        "name": "Georgia Chalvatzaki",
        "affiliation": "TU Darmstadt",
        "website": "https://irosalab.com/people/georgia-chalvatzaki/",
        "img": "imgs/georgia.jpg"
      },
      {
        "name": "Beomjoon Kim",
        "affiliation": "KAIST",
        "website": "https://beomjoonkim.github.io/",
        "img": "imgs/beomjoon.jpg"
      },
      {
        "name": "Eric Rosen",
        "affiliation": "Boston Dynamics AI Institute",
        "website": "https://cs.brown.edu/people/er35/home.html",
        "img": "imgs/eric.jpg"
      },
      {
        "name": "David Paulius",
        "affiliation": "Brown University",
        "website": "https://davidpaulius.github.io/",
        "img": "imgs/david.jpg"
      },
      {
        "name": "Naman Shah",
        "affiliation": "Arizona State University",
        "website": "https://www.namanshah.net/",
        "img": "imgs/naman.jpg"
      },
      {
        "name": "Tom Silver",
        "affiliation": "MIT",
        "website": "https://web.mit.edu/tslvr/www/",
        "img": "imgs/tom.jpg"
      }
    ];

    function randomize_organizers() {
      // Source: https://stackoverflow.com/a/6274381
      function shuffle(a) {
        var j, x, i;
        for (i = a.length - 1; i > 0; i--) {
          j = Math.floor(Math.random() * (i + 1));
          x = a[i];
          a[i] = a[j];
          a[j] = x;
        }
        return a;
      }

      // randomly shuffling order of organizers upon load!
      shuffle(organizers);

      for (var x = 0; x < organizers.length; x++) {
        let img_element = document.createElement("img");
        img_element.src = organizers[x].img;
        img_element.alt = organizers[x].name;

        let name_element = document.createElement("p");
        let strong_element = document.createElement("strong");
        let p = document.createElement("p");
        let a = document.createElement("a");
        a.href = organizers[x].website;
        a.target = "_blank";
        a.textContent = organizers[x].name;
        p.appendChild(a);
        strong_element.appendChild(p);
        name_element.appendChild(strong_element);

        let aff_element = document.createElement("p");
        aff_element.textContent = organizers[x].affiliation;

        let thumbnail = document.createElement("div");
        thumbnail.classList.add("thumbnail")
        thumbnail.appendChild(img_element);
        thumbnail.appendChild(name_element);
        thumbnail.appendChild(aff_element);

        let column = document.createElement("div");
        column.classList.add("col-sm-4")
        column.appendChild(thumbnail);

        if (x < organizers.length / 2) {
          document.getElementById("orgrow_1").appendChild(column);
        } else {
          document.getElementById("orgrow_2").appendChild(column);
        }
      }
    }
  </script>

</head>

<body>
  <nav class="navbar navbar-expand-xl navbar-expand-lg navbar-expand-custom navbar-fixed-top sticky-nav">
    <button class="navbar-toggler navbar-light" type="button" data-toggle="collapse" data-target="#main-navigation">
      <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="main-navigation">
      <ul class="navbar-nav">
        <li class="nav-item">
          <a class="nav-link" href="index.html#">Home</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="index.html#call">Call for Papers</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="index.html#dates">Dates</a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="index.html#schedule">Schedule</a>
        </li>
        <!-- <li class="nav-item">
            <a class="nav-link" href="index.html#program">Program</a>
          </li> -->
        <!-- <li class="nav-item">
          <a class="nav-link" href="index.html#related">Related Workshops</a>
        </li> -->
        <li class="nav-item">
          <a class="nav-link" href="index.html#committee">Committees</a>
        </li>
      </ul>
    </div>
  </nav>

  <div class="title-container">
    <div style="text-align: center;">
      <a href="https://www.corl2023.org/" target="_blank"><img src="imgs/corl2023.png" style="width:250px;"></a>
      <br>
      <div class="subtitle">1st Workshop on </div>
      <h1 style="width:80%;margin:auto;">Learning Effective Abstractions for Planning (LEAP)</h1>
      <div class="subtitle" style="color: #5a0b69; margin: 20px; margin-bottom: -10px;">
        CoRL 2023, Atlanta, GA, USA | November 6, 2023
      </div>
      <br>
      <h3>Email: <a href="mailto:leap-workshop@googlegroups.com">leap-workshop@googlegroups.com</a></h3>
    </div>
  </div>

  <div class="container" style="padding-bottom: 0px;">
    <div class="section" id="overview">
      <h2>Overview</h2>
      <p>
        Despite significant recent advances, planning remains a fundamentally hard problem, especially when
        considering robotic applications with long-horizon tasks, sparse feedback, and continuous state and action
        spaces. Abstraction is one of the main tools we have to overcome such challenges. State
        abstraction allows agents to focus on the important aspects of a planning problem, and action abstraction
        allows agents to reason across long horizons and exploit hierarchy in task execution. Designing abstractions
        by hand is labor-intensive and difficult: even for an expert, it is difficult to anticipate the influence of
        the abstraction on planning performance.
        <i>This motivates learning abstractions for efficient and effective robotic planning.</i>
      </p>

      <p>
        This workshop will bring together researchers from several related but often disjoint subcommunities who
        share an interest in learning abstractions for robotic planning. Key questions for discussion include:
      <ol>
        <li>
          What is the right objective for abstraction learning for robotic planning? To what extent should we
          consider soundness, completeness, planning efficiency, task distributions?
        </li>
        <li>
          To what extent should the abstractions used for robotic planning be interpretable or explainable to
          a human? How can this be achieved?
        </li>
        <li>
          When, where, and from what data should abstractions be learned? In the robot factory once and for
          all? In the ‚Äúwild‚Äù from interaction with humans or the world?
        </li>
        <li>
          How can and should pretrained language models (e.g., GPT-4) and vision-language models (e.g., CLIP)
          be leveraged towards abstraction learning for robotic planning?
        </li>
        <li>
          To what extent is natural language a useful representation for abstraction learning? What are the
          virtues of alternative or additional representations?
        </li>
      </ol>
      </p>

      <p>
        This workshop comes at a pivotal time as the field works to understand the implications of large pretrained
        language and vision models for robotic planning. As suggested by the latter discussion questions, these
        foundation models will be a central workshop theme. We believe there are rich opportunities not only for
        foundation models to aid robotic planning, but also for robotic planning research to inform the further
        development of foundation models. For example, the ‚Äúright objective‚Äù for abstraction learning could be used
        for foundation model training if those models are ultimately meant for robotic planning.
      </p>
    </div>

    <div class="section" id="objectives">

      <h2 style="text-align: center;">Objectives of LEAP Workshop</h2>
      <p>
        Current research on abstraction learning for robotic planning is fragmented across several subcommunities
        including task and motion planning, reinforcement learning (hierarchical, model-based), planning with linear
        temporal logic, planning with natural language, and neuro-symbolic AI. This workshop aims to create a common
        forum to share insights, discuss key questions, and chart a path forward. For this to succeed, we will need
        to establish a shared understanding of what abstraction means in the context of robot planning. This will
        require bridge-building between different sub-disciplines, which we will facilitate in two ways: <b>(1) a
          diverse selection of papers and invited speakers</b>; and <b>(2) a highly interactive workshop</b>.
      </p>

      <h3 style="text-align: center;">Areas of Interest</h3>
      <p>We solicit papers of the following topics:
      <ul>
        <li>Learning state abstractions and action abstractions</li>
        <li>Natural language as an abstraction for learning-based planning</li>
        <li>Learning other knowledge representations for planning</li>
        <li>Learning for planning with linear temporal logic (LTL)</li>
        <li>Learning for task and motion planning (TAMP)</li>
        <li>Learning for hierarchical planning</li>
        <li>Neuro-symbolic approaches for planning</li>
        <li>Hierarchical reinforcement learning</li>
        <li>Model-based reinforcement learning with latent representations</li>
        <li>Large language models (LLMs) and/or Foundation Models for Robot Planning </li>
      </ul>
      </p>
    </div>

    <!-- <div class="section" id="call" style="padding:0px 0x;" >
      <h2>Call for Papers</h2>
      <p>
        Current research on abstraction learning for robotic planning is fragmented across several subcommunities
        including task and motion planning, reinforcement learning (hierarchical, model-based), planning with linear
        temporal logic, planning with natural language, and neuro-symbolic AI.

        This workshop aims to create a common forum to share insights, discuss key questions, and chart a path forward
        for abstraction learning. For this to succeed, we will need to establish a shared understanding of what
        abstraction means in the context of robot planning. This will require bridge-building between different
        sub-disciplines, which we will facilitate in two ways: (1) a diverse selection of papers and invited speakers;
        and (2) a highly interactive workshop.
      </p>
    </div>
 -->
    <div class="section" id="call">

      <h2>Submission Guidelines</h2>

      <p>
        We solicit workshop paper submissions relevant to the above call of the following types:
      <ul>
        <li> Long papers - up to 8 pages plus unlimited references / appendices</li>
        <li> Short papers - up to 4 pages plus unlimited references / appendices</li>
      </ul>
      </p>

      <p>Please format submissions in CoRL or IEEE conference (ICRA or IROS) styles. Submissions do not need to be anonymized. To authors submitting papers
        rejected from other conferences: please ensure that comments given by the reviewers are addressed prior to
        submission.
      </p>

      <p>
        <i>Note: Please feel free to submit work under review or accepted for presentation at other workshops and/or
          conferences as we will
          not require copyright transfer.</i>
      </p>
      <br>
      <h3 style="text-align: center;"><b>Accepted papers can be found on <a href="https://openreview.net/group?id=robot-learning.org/CoRL/2023/Workshop/LEAP">OpenReview</a>.</b></h3>
      <h3 style="text-align: center;"><b>Best paper: <a href="https://openreview.net/forum?id=k2vlJyncri">Universal Visual Decomposer: Long-Horizon Manipulation Made Easy </a></b></h3>
      <center><img src="imgs/certificate.png" width="500" /></center>
    </div>
  </div>

  </div>



  <div class="section" id="dates" style="align-content: center;">
    <h2>Important Dates</h2>
    <table class="calculator table-borderless" style="text-align:center; margin: 0 auto;">
      <tr>
        <td class="noborder">Announcement and Call for Submissions
        </td>
        <td class="noborder"><b class="font-weight-bold">Aug 1, 2023</b>
        </td>
      </tr>
      <tr>
        <td class="noborder">Paper Submission Deadline
        </td>
        <td class="noborder"><b class="font-weight-bold">September 30, 2023</b> (11:59 PM UTC-12)
        </td>
      </tr>
      <tr>
        <td class="noborder">Paper Acceptance
        </td>
        <td class="noborder"><b class="font-weight-bold">October 13, 2023</b>
        </td>
      </tr>
      <tr>
        <td class="noborder">Camera-ready Version Due
        </td>
        <td class="noborder"><b class="font-weight-bold">November 4, 2023</b> (11:59 PM UTC-12)
        </td>
      </tr>
      <tr>
        <td class="noborder">Workshop
        </td>
        <td class="noborder"><b class="font-weight-bold">November 6, 2023</b>
        </td>
      </tr>
    </table>
  </div>

  <div class="section" id="schedule" style="align-content: center;">
    <h2>Schedule</h2>
    <table class="calculator table-borderless" style="text-align:center; margin: 0 auto;">
      <tr>
        <td class="noborder"><b class="font-weight-bold">Time</b>
        </td>
        <td class="noborder"><b class="font-weight-bold">Session Type</b>
        </td>
        <td class="noborder"><b class="font-weight-bold">Speakers</b>
        </td>
        <td class="noborder"><b class="font-weight-bold">Portraits</b>
        </td>
      </tr>
      <tr>
        <td class="noborder">8:30 - 8:45
        </td>
        <td class="noborder">Welcome
        </td>
        <td class="noborder">LEAP Organizers
        </td>
        <td class="noborder">
        </td>
      </tr>
      <tr>
        <td class="noborder">8:45 - 9:00
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
            Title: Explore to Generalize
            <br><br>
            Abstract: The importance of learning abstractions for planning is well reflected in the zero shot generalization problem in RL: how to learn a policy on a set of training tasks that will generalize well to an unseen, but similar, test task. While most previous efforts focused on learning policies that are invariant to irrelevant properties of the task, we propose a different approach. We observe that policies trained to *explore* the state space generalize well, as the exploration behavior is much harder to memorize than the actual solution to the task. We exploit this discovery in our Explore-to-Generalize (ExpGen) algorithm, which at test time takes a reward maximization action when it is certain (using a policy ensemble), or an exploration action when it is uncertain (using the generalizing exploration policy). On the popular ProcGen benchmark, ExpGen makes significant progress on difficult problems such as Maze and Heist, where previous approaches failed. Furthermore, when combined with an invariance based method, ExpGen yields new SOTA results on the full suite of ProcGen games. (based on a NeurIPS 2023 paper by Zisselman, Lavie, Soudry, and T., <a href="https://openreview.net/forum?id=37cADkATD0">[link]</a>)            </div>
      </td>
        <td class="noborder">Aviv Tamar (virtual)
        </td>
        <td class="noborder">
          <img src="https://avivt.github.io/avivt/images/my_pic_small.png" class="rounded-circle"
          alt="Aviv Tamar" height="140">
        </td>
      </tr>
      <tr>
        <td class="noborder">9:00 - 9:15
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
            Title: Symbolic State Space Optimization: Learning Abstractions for Mobile Manipulation
            <br><br>
            Abstract: In existing task and motion planning (TAMP) research, it is a common assumption that experts manually specify the state space for task-level planning. A well-developed state space enables the desirable distribution of limited computational resources between task planning and motion planning. However, developing such task-level state spaces can be non-trivial in practice. In this paper, we consider a long horizon mobile manipulation domain including repeated navigation and manipulation. We propose Symbolic State Space Optimization (S3O) for computing a set of abstracted locations and their 2D geometric groundings for generating task-motion plans in such domains. Our approach has been extensively evaluated in simulation and demonstrated on a real mobile manipulator working on clearing up dining tables. Results show the superiority of the proposed method over TAMP baselines in task completion rate and execution time. (Joint work with Xiaohan Zhang, Yifeng Zhu, Yan Ding, Yuqian Jiang, Yuke Zhu, and Peter Stone).
          </div>
        </td>
        <td class="noborder">Shiqi Zhang (in person)
        </td>
        <td class="noborder">
        <img src="https://www.cs.binghamton.edu/~szhang/images/recent.jpg" class="rounded-circle"
        alt="Shiqi Zhang" height="140">
        </td>
      </tr>
      <tr>
        <td class="noborder">9:15 - 9:30
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
            <strong>Title</strong>: Transition model abstractions for efficient and reliable planning
              <br><br>
              Abstract: Though imperfect abstractions of the real world, transition models are still useful for planning. This talk explores how to use real-world data to inform in which contexts, which we call ‚Äúmodel preconditions‚Äù, a model should be used to balance computational efficiency with robust execution. The results show that by selecting between incomplete analytical models and a simulator, a robot can achieve faster planning and more reliable execution of those plans. We also investigate how robots can efficiently collect data to most effectively plan with imperfect models, such as by adapting a learned model or learning a model precondition.
          </div>
        </td>
        <td class="noborder">Alex LaGrassa (in person)
        </td>
        <td class="noborder">
          <img src="./imgs/alex_lagrassa_ri_.jpg" class="rounded-circle"
          alt="Alex LaGrassa" height="140">
          </td>
      </tr>
      <tr>
        <td class="noborder">9:30 - 9:45
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
          Title: Value-Based Abstractions for Planning
          <br><br>
          Abstract: As the field of robotics continues to advance, the integration of efficient planning algorithms with powerful representation learning becomes crucial for enabling robots to perform complex manipulation tasks. We address key challenges in planning, reward learning, and representation learning through the objective of learning value-based abstractions. We explore this idea via goal-conditioned reinforcement learning, action-free pre-training, and with language. By leveraging self-supervised reinforcement learning and efficient planning algorithms, these approaches collectively contribute to the advancement of robotic systems capable of learning and adapting to diverse tasks in real-world environments.          </div>
        </td>
        <td class="noborder">Amy Zhang (virtual)
        </td>
        <td class="noborder">
          <img src="https://pbs.twimg.com/profile_images/949764133922238465/homPC1W5_400x400.jpg" class="rounded-circle"
          alt="Amy Zhang" height="140">
          </td>
      </tr>
      <tr>
        <td class="noborder">9:45 - 10:30
        </td>
        <td class="noborder"> Panel
        </td>
        <td class="noborder">Aviv, Shiqi, Alex, Amy (hybrid)
        </td>
        <td class="noborder">
        </td>
      </tr>
      <tr>
        <td class="noborder">10:30 - 11:00
        </td>
        <td class="noborder"> Coffee Break
        </td>
        <td class="noborder">N/A
        </td>
        <td class="noborder">
        </td>
      </tr>
      <tr>
        <td class="noborder">11:00-11:15
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
            Title: Interactive Task Learning
<br><br>
Abstract: Robots in manufacturing and households will encounter novel objects and tasks all the time. This talk focuses on teaching robots novel concepts and tasks human-robot interactions, which can include demonstrations and linguistic interactions. We will discuss challenges in teaching tasks and plannable representations to robots with human demonstrations. We will present results on the types of teaching strategies people use when teaching tasks to robots. We will then present a method that can learn visual tasks and concepts from a few in-situ interactions with a human-teacher using interactive learning. Together these approaches will shed light on the difficulties in teaching generalizable task and motion planning abstractions to a robot in a continual setting.
            </div>
      </td>
        <td class="noborder"> Nakul Gopalan (in person)
        </td>
        <td class="noborder">
          <img src="https://fullcircle.asu.edu/wp-content/uploads/2022/08/Nakul-Gopalan-01a-600x600-1.jpg" class="rounded-circle"
          alt="Nakul Gopalan" height="140">
        </td>
      </tr>
      <tr>
        <td class="noborder">11:15-11:30
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
            Title: How (temporal) reward abstractions help planning and task understanding
            <br><br>
            Abstract: Temporal reward abstractions frameworks such as linear temporal logic help with generating a compact description of acceptable trajectories of the world state. While these reward descriptions when translated to automata can help speed up planning and learning by providing a reward scaffold, this talk explores some other utilities of reward definitions using beliefs over LTL formulas. Specifically, I will talk about three use cases. First, where belief over LTL specifications can be used to generate active learning query trajectories are used to prune the belief space over task specifications efficiently, allowing the robot to learn high quality task specifications even if starting from a highly uncertain belief. Next, we demonstrate how the automata structure of the rewards can help with zero-shot policy transfer to a novel task, and finally, we demonstrate how models that translate natural language to formal specifications can leverage compositional structure to adapt to novel domains without any additional domain specific training.</div>
      </td>
        <td class="noborder"> Ankit Shah (in person)
        </td>
        <td class="noborder">
          <img src="http://people.csail.mit.edu/ajshah/authors/ankit/avatar_hue4f007176f1b7507a4fad3a8f218276e_7283443_250x250_fill_q90_lanczos_center.jpg" class="rounded-circle"
          alt="Ankit Shah" height="140">
          </td>
      </tr>
      <tr>
        <td class="noborder">11:30-11:45
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
            Title: DeepSym: Use of Symbolic Bottleneck for Extracting High-Level Abstractions from Robots Interactions
            <br><br>
            Abstract: Symbolic planning and reasoning are powerful tools for robots tackling complex tasks. However, the need to manually design the symbols restrict their applicability, especially for robots that are expected to act in open-ended environments. Therefore symbol formation and rule extraction should be considered part of robot learning, which, when done properly, will offer scalability, flexibility, and robustness.  Towards this goal, we propose a novel general method that finds action-grounded, discrete object and effect categories and builds probabilistic rules over them for non-trivial action planning.
                        </div>
      </td>
        <td class="noborder"> Alper Ahmetoglu (virtual)
        </td>
        <td class="noborder">
          <img src="https://alpera.xyz/static/alper.jpg" class="rounded-circle"
          alt="Alper Ahmetoglu" height="140">
          </td>
      </tr>
      <tr>
        <td class="noborder">11:45-12:30
        </td>
        <td class="noborder"> Panel
        </td>
        <td class="noborder"> Nakul, Ankit, Alper (hybrid)
        </td>
        <td class="noborder">
        </td>
      </tr>
      <tr>
        <td class="noborder">12:30-1:45
        </td>
        <td class="noborder"> Lunch
        </td>
        <td class="noborder"> N/A
        </td>
        <td class="noborder">
        </td>
      </tr>
        <td class="noborder">1:45-2:00
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Contributed Talk (Details)</u><div class="triangle-down"></div></button>
          <div class="content">Generative Skill Chaining: Long-Horizon Skill Planning with Diffusion Models
          <br><br>Utkarsh Mishra, Shangjie Xue, Yongxin Chen, Danfei Xu
          </div>
        </td>
        <td class="noborder"> <a href="https://openreview.net/forum?id=dQtjGWt1D2">[paper link]</a>
        </td>
        <td class="noborder">
        </td>
      </tr>
    </tr>
    <td class="noborder">2:00-2:15
    </td>
    <td class="noborder"> <button type="button" class="collapsible"><u>Contributed Talk (Details)</u><div class="triangle-down"></div></button>
      <div class="content">Universal Visual Decomposer: Long-Horizon Manipulation Made Easy
      <br><br> Zichen Zhang, Yunshuang Li, Osbert Bastani, Abhishek Gupta, Dinesh Jayaraman, Yecheng Jason Ma, Luca Weihs
      </div>
    </td>
    <td class="noborder"> <a href="https://openreview.net/forum?id=k2vlJyncri">[paper link]</a>
    </td>
    <td class="noborder">
    </td>
  </tr>
</tr>
<td class="noborder">2:15-2:30
</td>
<td class="noborder"> <button type="button" class="collapsible"><u>Contributed Talk (Details)</u><div class="triangle-down"></div></button>
  <div class="content">NOD-TAMP: Multi-Step Manipulation Planning with Neural Object Descriptors
  <br><br> Shuo Cheng, Caelan Garrett, Ajay Mandlekar, Danfei Xu
  </div>
</td>
<td class="noborder"> <a href="https://openreview.net/forum?id=DK7TbAS0Wz">[paper link]</a>
</td>
<td class="noborder">
</td>
</tr>
</tr>
<td class="noborder">2:30-2:45
</td>
<td class="noborder"> <button type="button" class="collapsible"><u>Contributed Talk (Details)</u><div class="triangle-down"></div></button>
  <div class="content"> RoboHop: Segment-based Topological Map Representation for Open-World Visual Navigation
  <br><br> Sourav Garg, Krishan Rana, Mehdi Hosseinzadeh, Lachlan Mares, Niko Suenderhauf, Feras Dayoub, Ian Reid
  </div>
</td>
<td class="noborder"> <a href="https://openreview.net/forum?id=DFPjxfmdep">[paper link]</a>
</td>
<td class="noborder">
</td>
</tr>
      <tr>
        <td class="noborder">2:45-3:00
        </td>
          <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
            <div class="content">
              Title: Causal Dynamics Learning for Task-Independent State Abstraction
              <br><br>
Abstract:  Learning dynamics models accurately is an important goal for
 Model-Based Reinforcement Learning (MBRL), but most MBRL methods learn
 a dense dynamics model which is vulnerable to spurious correlations
 and therefore generalizes poorly to unseen states. We introduce Causal
 Dynamics Learning for Task-Independent State Abstraction (CDL), which
 first learns a theoretically proved causal dynamics model that removes
 unnecessary dependencies between state variables and the action, thus
 generalizing well to unseen states. A state abstraction can then be
 derived from the learned dynamics, which not only improves sample
 efficiency but also applies to a wider range of tasks than existing
 state abstraction methods. Evaluated on two simulated environments and
 downstream tasks, both the dynamics model and policies learned by the
 proposed method generalize well to unseen states and the derived state
 abstraction improves sample efficiency compared to learning without
 it.
 (Based on joint work with Zizhao Wang, Xuesu Xiao, Zifan Xu, and Yuke Zhu)            </div>
        </td>
        <td class="noborder"> Peter Stone (virtual)
        </td>
        <td class="noborder">
          <img src="https://www.cs.utexas.edu/~pstone/images/Peter_Headshot_2015.png" class="rounded-circle"
          alt="Peter Stone" height="140">
          </td>
      </tr>
      <tr>
        <td class="noborder">3:00-3:15
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
            Title: A first-order language for effective task-specific abstractions
            <br><br>
            Abstract: Domain-specific knowledge can be leveraged to deduce task-specific abstractions for various sequential decision-making domains.  In this talk, I argue the necessity of a relational, first-order language to express the domain knowledge and present a dynamic first-order conditional influence language. I will show the connection of this language with the dynamic probabilistic logic models, present an approach to deduce the task-specific abstraction using that language, and demonstrate its utility in different sequential decision-making problems..
          </div>
      </td>
        <td class="noborder"> Harsha Kokel (virtual)
        </td>
        <td class="noborder">
          <img src="https://harshakokel.com/images/me.jpg" class="rounded-circle"
          alt="Harsha Kokel" height="140">
        </td>
      </tr>
      <tr>
        <td class="noborder">3:15-3:30
        </td>
        <td class="noborder"> <button type="button" class="collapsible"><u>Invited Speaker (Details)</u><div class="triangle-down"></div></button>
          <div class="content">
            Learning Category-level Sensorimotor Primitives of Sequential Manipulation Tasks from Visual Demonstrations
            <br><br>
            Abstract: We present a new approach for learning complex robot manipulation tasks that are composed of several, consecutively executed low-level sub-tasks, given as input a few visual demonstrations. The sub-tasks consist of moving the robot‚Äôs end-effector until it reaches a sub-goal region in the task space, performing an action, and triggering the next sub-task when a pre-condition is met. The proposed system learns simultaneously low-level policies as well as high-level policies, such as deciding which object to pick next or where to place it relative to other objects in the scene. A key feature of the proposed approach is that the policies are learned without any manual annotation or postprocessing of the data. We also present a category-level learning approach that aims to acquire skills that can be generalized to new objects, with geometries and textures that are different from the ones of the objects used in the demonstrations.
          </div>
      </td>
        <td class="noborder"> Abdeslam Boularias (virtual)
        </td>
        <td class="noborder">
          <img src="https://www.cs.rutgers.edu/images/people/photo%20(5).jpeg" class="rounded-circle"
          alt="Abdeslam Boularias" height="140">
          </td>
      </tr>
      <tr>
        <td class="noborder">3:30-3:35
        </td>
        <td class="noborder"> Conclusion & Best Paper
        </td>
        <td class="noborder"> LEAP Organizers
        </td>
        <td class="noborder">
        </td>
      </tr>
      <tr>
        <td class="noborder">3:35-5:30
        </td>
        <td class="noborder"> Posters / Coffee
        </td>
        <td class="noborder"> N/A
        </td>
        <td class="noborder">
        </td>
      </tr>
    </table>
  </div>



  <div class="section" id="committee" style="text-align: center; padding: 10px;">
    <h2>Committees</h2>
    <h3>Organizing Committee</h3>
    <br />

    <div class="grid">
      <center>
        <div class="row">
          <div class="col-lg-4 col-md-4 col-sm-4 col-xs-12" style="padding-bottom: 30px;">
            <a href="https://irosalab.com/people/georgia-chalvatzaki/" target="_blank"><img src="imgs/georgia.jpg" class="rounded-circle"
                alt="Georgia Chalvatzaki" width="140" height="140"></a>
            <h5 style="margin-bottom:0em;"><a href="https://irosalab.com/people/georgia-chalvatzaki/" target="_blank">Georgia Chalvatzaki</a>
            </h5>
            TU Darmstadt, Germany
          </div>
          <br /> <br />
          <div class="col-lg-4 col-md-4 col-sm-4 col-xs-12" style="padding-bottom: 30px;">
            <a href="https://beomjoonkim.github.io/" target="_blank"><img src="imgs/beomjoon.jpg" class="rounded-circle"
                alt="Beomjoon Kim" width="140" height="140"></a>
            <h5 style="margin-bottom:0em;"><a href="https://beomjoonkim.github.io/" target="_blank">Beomjoon Kim</a>
            </h5>
            KAIST, South Korea
          </div>
          <br /> <br />
          <div class="col-lg-4 col-md-4 col-sm-4 col-xs-12" style="padding-bottom: 30px;">
            <a href="https://www.davidpaulius.me/" target="_blank"><img src="imgs/david.jpg" class="rounded-circle"
                alt="David Paulius" width="140" height="140"></a>
            <h5 style="margin-bottom:0em;"><a href="https://davidpaulius.github.io/" target="_blank">David Paulius</a>
            </h5>
            Brown University, RI, USA
          </div>
          <br /><br />
          <div class="col-lg-4 col-md-4 col-sm-4 col-xs-12" style="padding-bottom: 30px;">
            <a href="https://eric-rosen.github.io/" target="_blank"><img src="imgs/eric.png" class="rounded-circle"
                alt="Eric Rosen" width="140" height="140"></a>
            <h5 style="margin-bottom:0em;"> <a href="https://eric-rosen.github.io/" target="_blank">Eric Rosen</a>
            </h5>
            Boston Dynamics AI Institute, MA, USA
          </div>
          <br /> <br />
          <div class="col-lg-4 col-md-4 col-sm-4 col-xs-12" style="padding-bottom: 30px;">
            <a href="https://namanshah.net/" target="_blank"><img src="imgs/naman.jpg" class="rounded-circle"
                alt="Naman Shah" width="140" height="140"></a>
            <h5 style="margin-bottom:0em;"> <a href="https://namanshah.net/" target="_blank">Naman Shah</a></h5>
            Arizona State University, AZ, USA
          </div>
          <br /> <br />
          <div class="col-lg-4 col-md-4 col-sm-4 col-xs-12" style="padding-bottom: 30px;">
            <a href="https://web.mit.edu/tslvr/www/" target="_blank"><img src="imgs/tom.jpg" class="rounded-circle"
                alt="Tom Silver" width="140" height="140"></a>
            <h5 style="margin-bottom:0em;"> <a href="https://web.mit.edu/tslvr/www/" target="_blank">Tom Silver</a>
            </h5>
            Massachusetts Institute of Technology, MA, USA
          </div>
          <br /> <br />
        </div>
      </center>
    </div>
    <br /><br />

    <h3 style="text-align: center;">Advisory Board</h3>
    <p><a href="https://people.csail.mit.edu/lpk/">Leslie Pack Kaelbling</a> - Massachusetts Institute of Technology,
      USA</p>
    <p><a href="https://cs.brown.edu/~gdk/">George Konidaris</a> - Brown University, USA</p>
    <p><a href="http://siddharthsrivastava.net" target="_blank">Siddharth Srivastava</a> - Arizona State University,
      USA.</p>


  </div>
  </div>

  <script src="javascripts/scale.fix.js"></script>
  <script>
    var coll = document.getElementsByClassName("collapsible");
    var i;

    for (i = 0; i < coll.length; i++) {
      coll[i].addEventListener("click", function() {
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.display === "block") {
          content.style.display = "none";
        } else {
          content.style.display = "block";
        }
      });
    }
    </script>
</body>

</html>
