---
id: Overview
name: Overview
heading: Overview
div_class: lead
# subheading: Will Catch Your Eye
# image: "http://placehold.it/500x500"
---

Audience Q&A: [https://app.sli.do/event/aPSF8kUkZXDnNSXByoQvev/live/questions](https://app.sli.do/event/aPSF8kUkZXDnNSXByoQvev/live/questions)

Robots now have advanced perception, navigation, grasping and manipulation capabilities, but how come it's still exceedingly difficult to bring these skills together to get a robot to autonomously tidy a room? A key limiting factor is that robots still lack the contextual scene understanding capabilities that allow humans to efficiently and compactly reason about our world and our actions within it. Metric (where) and semantic (what) representations are now common, but contextual (how) representations–how do objects interrelate and how can a robot interact with objects to achieve the task?–are still missing. How should we formulate these representations, and crucially, how can we allow robots–embodied agents–learn and update their contextual scene understanding from live experiences? Researchers in AI knowledge representation and reasoning as well as in the more distant field of linguistics have long grappled with similar questions. The goal of this workshop is to bring together those experts with researchers in the fields of robot scene understanding and long-horizon planning to discuss the state of the art and uncover synergies across the currently disparate disciplines.

